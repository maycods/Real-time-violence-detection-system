{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "import cv2\n",
    "import base64\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_url=\"https://www.youtube.com/embed/XjhxRZEm638?rel=0\"\n",
    "youtube_url2=\"https://www.youtube.com/embed/NdW8Mh0lmOo\"\n",
    "youtube_url3=\"https://www.youtube.com/embed/u4UZ4UvZXrg\"\n",
    "\n",
    "embed_html1 = f\"\"\"\n",
    "  <div style=\"position: relative; padding-bottom: 93%;\">\n",
    "    <iframe src=\"{youtube_url}\" style=\"width: 100%; height: 100%; position: absolute; border: 0;\" allowfullscreen scrolling=\"no\" allow=\"accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share;\"></iframe>\n",
    "  </div>\"\"\"\n",
    "embed_html2= f\"\"\"<div style=\"position: relative; padding-bottom: 93%;\">\n",
    "    <iframe src=\"{youtube_url2}\" style=\"width: 100%; height: 100%; position: absolute; border: 0;\" allowfullscreen scrolling=\"no\" allow=\"accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share;\"></iframe>\n",
    "  </div>\n",
    "  \"\"\"\n",
    "embed_html3= f\"\"\"\n",
    " <div style=\"position: relative; padding-bottom: 93%;\">\n",
    "    <iframe src=\"{youtube_url3}\" style=\"width: 100%; height: 100%; position: absolute; border: 0;\" allowfullscreen scrolling=\"no\" allow=\"accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share;\"></iframe>\n",
    "  </div>\n",
    " \"\"\"\n",
    "custom_css = \"\"\"\n",
    "body{\n",
    "margin : 0px;\n",
    "}\n",
    ".gradio-container {\n",
    "background-color: #092327 ;\n",
    "margin:0px;\n",
    "}\n",
    "    .image-upload{\n",
    "         display: flex;\n",
    "        flex-direction: column;\n",
    "        flex-grow: 1;\n",
    "    }\n",
    "     .image-upload2{\n",
    "        flex-grow: 1;\n",
    "    }\n",
    "  \n",
    "#Detection {\n",
    "    background-color: #0B5351;\n",
    "}\n",
    "\n",
    ".svelte-1mhtq7j{\n",
    "  background-color:#092327;\n",
    "  background-image: linear-gradient(to bottom,#092327,#092327), linear-gradient(to bottom,#092327,#092327);\n",
    "  background-clip: content-box, padding-box;\n",
    "\n",
    "}\n",
    ".svelte-1mhtq7j:hover{\n",
    "  background-color:#00A9A5 ;\n",
    "  background-image: linear-gradient(to bottom,#00A9A5,#00A9A5 ), linear-gradient(to bottom,#00A9A5,#00A9A5);\n",
    "  background-clip: content-box, padding-box;\n",
    "\n",
    "}\n",
    ".svelte-1mhtq7j:checked {\n",
    "  background-color:#00A9A5 ;\n",
    "  background-image: linear-gradient(to bottom,#00A9A5,#00A9A5 ), linear-gradient(to bottom,#00A9A5,#00A9A5);\n",
    "  background-clip: content-box, padding-box;\n",
    "\n",
    "}\n",
    "#Detection textarea{\n",
    "    background-color: #00A9A5;\n",
    "}\n",
    "\n",
    "#Detection.changed textarea{\n",
    "    background-color: #00A9A5;\n",
    "}\n",
    "#Detection.changed2 textarea{\n",
    "    background-color: #D32F2F;\n",
    "}\n",
    "#Detection.changed2 {\n",
    "    background-color: #D32F2F;\n",
    "}\n",
    "textarea {\n",
    "    font-size: 20px;\n",
    "    background-color:#092327;\n",
    "}\n",
    ".svelte-1gfkn6j{\n",
    "font-size:20px;\n",
    "}\n",
    "#Detection label{\n",
    "    font-size: 15px;\n",
    "    font-weight: bold;\n",
    "    background-color:#0B5351 ;\n",
    "}\n",
    "#Detection input[type=number]{\n",
    "    text-size:20;\n",
    "    background-color:#092327 ;\n",
    "}\n",
    ".svelte-1uw5tnk {\n",
    "    font-size: 20px;\n",
    "    font-weight: bold;\n",
    "}\n",
    ".lg secondary{\n",
    "        display: block;\n",
    "        margin: auto;\n",
    "        font-size:15;\n",
    "        background-color:#092327 !important;\n",
    "    }\n",
    ".svelte-cmf5ev{\n",
    "        display: block;\n",
    "        margin: auto;\n",
    "        font-size:15;\n",
    "        background-color:#092327 !important;\n",
    "    }\n",
    "\n",
    "}\n",
    ".btn {\n",
    "font-size: 18px !important;\n",
    "    background-color: lightblue !important;\n",
    "    color: darkblue !important;\n",
    "    \n",
    "    border: none;\n",
    "    padding: 10px 20px;\n",
    "    cursor: pointer;\n",
    "}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i=0\n",
    "model= tf.keras.models.load_model(\"cnn_final_final.keras\")\n",
    "rep=[0]\n",
    "fig=None\n",
    "def Detection(input):\n",
    "    global i,rep,fig\n",
    "    if i%3==0:\n",
    "        input_r =cv2.resize(input,(224,224))\n",
    "        prec=rep[0]\n",
    "        y_pred=model.predict(tf.keras.applications.mobilenet.preprocess_input(np.expand_dims(cv2.resize(input_r,(224,224)), axis=0)),verbose=0)\n",
    "        rep=np.array(y_pred)[:,1]>0.8\n",
    "\n",
    "        y_values =[np.array(y_pred)[:,0][0],np.array(y_pred)[:,1][0]]\n",
    "\n",
    "        plt.figure(figsize=(5,3))#\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.barh(y=[\"violence\",\"non violence\"], edgecolor='black',color=[\"#2a9d8f\",\"#244454\"],linewidth=1.5, alpha=0.9,width=y_values)\n",
    "        ax.tick_params(axis='x', which='major', labelsize=12)\n",
    "        ax.tick_params(axis='y', which='major', labelsize=12)\n",
    "        ax.grid(True, which='both', linestyle='--', linewidth=0.3, color='gray', alpha=0.7)\n",
    "        ax.set_axisbelow(True)\n",
    "\n",
    "\n",
    "    \n",
    "    if not(rep[0]==1):\n",
    "        result=  str(i)+\" normale\"  \n",
    "        audio_player = ''  \n",
    "        img=None\n",
    "        ch=\"changed\"\n",
    "    else:\n",
    "        result= str(i)+\" VIOLEEEEEEEEEEEENCE\"\n",
    "        ch=\"changed2\"\n",
    "        img=input\n",
    "        screenshot=cv2.cvtColor(input, cv2.COLOR_BGR2RGB)\n",
    "        cv2.imwrite(\"screenshots//\"+str(i)+\" violencee.png\", screenshot) \n",
    "        audio_file=open(\"alarms//medical_alarm.mp3\", 'rb')\n",
    "        audio_content = audio_file.read()\n",
    "        audio_base64 = base64.b64encode(audio_content).decode('utf-8')\n",
    "        audio_player = f'<audio src=\"data:audio/mpeg;base64,{audio_base64}\" autoplay controls></audio>'  \n",
    "    \n",
    "  \n",
    "    i=i+1\n",
    "    return gr.update(visible=True, value=result,elem_classes=ch),audio_player,img,fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hue=[21,40,75,155,190,270,295,315]\n",
    "Sat=[0,0.2,0.7,1.1]\n",
    "\n",
    "def Mat_adj(hists):\n",
    "    n = len(hists)\n",
    "    M = np.zeros((n, n), dtype=int)\n",
    "\n",
    "    for i in range(n):\n",
    "        M[i, :i] = np.linalg.norm(hists[i, None] - hists[:i], axis=1)\n",
    "\n",
    "    return M \n",
    "def local_density(M):\n",
    "    t=np.percentile(np.sort(M[M != 0.0].flatten()),2)\n",
    "\n",
    "    locs=[]\n",
    "    for i in range(len(M)):\n",
    "        dist = np.concatenate((M[i+1:,i], M[i,:i]))\n",
    "        loc = np.count_nonzero(dist <= t)\n",
    "        locs.append(loc)\n",
    "    return np.array(locs,dtype=np.float64)\n",
    "def higher(M,locs):\n",
    "    his=[]\n",
    "    max_loc=np.max(locs)\n",
    "    n=len(locs)\n",
    "    for i in range(n):\n",
    "        if locs[i]==max_loc:\n",
    "            hi= np.max(np.concatenate((M[i+1:,i], M[i,:i])))\n",
    "        else:\n",
    "            indexes=   np.where(locs > locs[i])[0]\n",
    "            hi=  np.min(np.concatenate((M[i,indexes[indexes < i]],M[indexes[indexes > i],i])))\n",
    "        his.append(hi)\n",
    "    return np.array(his,dtype=np.float64)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Extract_key_frames(video_path,thr, Constraint):#\n",
    "    imgs=[]\n",
    "    frames=[]\n",
    "    hists=[]\n",
    "    imgs2=[]\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise Exception(\"Error opening video file.\")      \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        raise Exception(\"Error reading first frame from video.\")  \n",
    "    \n",
    "    prev_frame = frame\n",
    "    prev_frame=cv2.cvtColor(prev_frame,cv2.COLOR_BGR2RGB) \n",
    "    imgs.append(prev_frame)  \n",
    "    ret, current_frame = cap.read()\n",
    "    while ret:\n",
    "        frames.append(cv2.cvtColor(current_frame,cv2.COLOR_BGR2RGB))\n",
    "        diff = cv2.absdiff(current_frame, prev_frame)\n",
    "        diff_count = np.count_nonzero(diff)    \n",
    "        if diff_count >= thr: \n",
    "            current_frame=cv2.cvtColor(current_frame,cv2.COLOR_BGR2RGB)       \n",
    "            imgs.append(current_frame)    \n",
    "        prev_frame = current_frame\n",
    "        ##\n",
    "        \n",
    "        current_frame = cv2.cvtColor(current_frame, cv2.COLOR_BGR2HSV)    \n",
    "        h,s,v=cv2.split(current_frame)\n",
    "        s= s.astype(np.float64) /255\n",
    "        v=v.astype(np.float64)/255\n",
    "        h,s,v=h.flatten(),s.flatten(),v.flatten()\n",
    "        F=9*(np.digitize(h, Hue)%8)+3*(np.digitize(s,Sat)-1)+np.digitize(v,Sat)-1\n",
    "        a=Counter(F)\n",
    "        left=set(range(72) )- set(a.keys())\n",
    "        a.update({x:0 for x in left})\n",
    "        values_in_order = [a[key] for key in sorted(a.keys())]     \n",
    "        hists.append(values_in_order)\n",
    "        ##\n",
    "        ret, current_frame= cap.read()\n",
    "    hists=np.array(hists)\n",
    "    cap.release()\n",
    "    M=Mat_adj(hists)\n",
    "    locs=local_density(M)\n",
    "    his=higher(M,locs)\n",
    "    locs/=np.max(locs)\n",
    "    his/=np.max(his)\n",
    "    both= np.column_stack([locs,his,np.arange(len(his), dtype=int)])\n",
    "    both = both[both[:,1].argsort()[::-1]]\n",
    "    _,idc=np.unique(both[:,0],return_index=True)\n",
    "    both= both[idc]\n",
    "    y=both[:,0]*both[:,1]\n",
    "    g=sorted(y)\n",
    "    y=y.tolist()\n",
    "    liste= [ y.index(i) for i in g[-Constraint:] ]# not enought att\n",
    "    a=np.array(sorted(both[liste,-1]),dtype=int)\n",
    "    f=[frames[i] for i in a]\n",
    "    for img in f:\n",
    "        imgs2.append(img)\n",
    "    return imgs,imgs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX \n",
    "org = (50, 50) \n",
    "fontScale = 1\n",
    "color = (255, 255, 255) \n",
    "thickness = 2\n",
    "def vd(video_path):#\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    model= tf.keras.models.load_model(\"cnn_final_final.keras\")\n",
    "    ret, current_frame = cap.read()\n",
    "    frame_cnt=0\n",
    "    yield gr.update(visible=False),None, gr.update(visible=True),None,None,None\n",
    "    \n",
    "    while ret:\n",
    "        current_frame=cv2.cvtColor(current_frame,cv2.COLOR_BGR2RGB)\n",
    "        if frame_cnt%30==0:\n",
    "            vd=\"non_violence\"\n",
    "            y_pred=model.predict(tf.keras.applications.mobilenet.preprocess_input(np.expand_dims(cv2.resize(current_frame,(224,224)), axis=0)),verbose=0)\n",
    "            rep=np.array(y_pred)[:,1]>0.9\n",
    "            audio_player = None\n",
    "            fig=None\n",
    "            ch=\"changed\"\n",
    "            plt.figure(figsize=(3,2))#\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.barh(y=[\"normale\",\"violence\"], edgecolor='black',color=[\"#2a9d8f\",\"#244454\"],linewidth=1.5, alpha=0.9,width=[np.array(y_pred)[:,0][0],np.array(y_pred)[:,1][0]])\n",
    "            plt.xlim(right=1)\n",
    "\n",
    "            ax.grid(True, which='both', linestyle='--', linewidth=0.3, color='gray', alpha=0.7)\n",
    "         \n",
    "            screenshot=None\n",
    "            if rep[0]==1:\n",
    "                vd=\"violence\"\n",
    "                ch=\"changed2\"\n",
    "                screenshot=current_frame\n",
    "                cv2.imwrite(\"screenshots//\"+str(frame_cnt)+\" violencee.png\", screenshot) \n",
    "                audio_file=open(\"alarms//medical_alarm.mp3\", 'rb')\n",
    "                audio_content = audio_file.read()\n",
    "                audio_base64 = base64.b64encode(audio_content).decode('utf-8')\n",
    "                audio_player = f'<audio src=\"data:audio/mpeg;base64,{audio_base64}\" autoplay controls></audio>'  \n",
    "    \n",
    "        yield  None,gr.update(visible=True, value=vd,elem_classes=ch) ,  current_frame,audio_player,fig,screenshot\n",
    "\n",
    "        ret, current_frame= cap.read()\n",
    "        \n",
    "        frame_cnt+=1\n",
    "    yield gr.update(visible=True),gr.update(visible=True, value=\"normale\",elem_classes=\"changed\"), gr.update(visible=False),None,None,None\n",
    " \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\gradio\\layouts\\column.py:53: UserWarning: 'scale' value should be an integer. Using 0.5 will cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTANT: You are using gradio version 4.19.1, however version 5.0.1 is available, please upgrade.\n",
      "--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\gradio\\queueing.py\", line 495, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\gradio\\route_utils.py\", line 233, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\gradio\\blocks.py\", line 1617, in process_api\n",
      "    data = await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\anyio\\to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\gradio\\blocks.py\", line 1385, in postprocess_data\n",
      "    predictions = convert_component_dict_to_list(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\gradio\\blocks.py\", line 448, in convert_component_dict_to_list\n",
      "    raise ValueError(\n",
      "ValueError: Returned component html not specified as output of function.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def fn(j):\n",
    "  if j==\"CAMERA 4\":\n",
    "    return {htm1:gr.HTML(embed_html1,visible=True),htm2:gr.HTML(embed_html2,visible=False),htm3:gr.HTML(embed_html3,visible=False),input:gr.Image(sources=[\"webcam\"], streaming=True,label=\"Camera de surveillance\",visible=False),c1:gr.Column(scale=1,visible=True),c2:gr.Column(scale=1,visible=False)}\n",
    "  elif j==\"CAMERA 2\":\n",
    "    return {htm1:gr.HTML(embed_html1,visible=False),htm2:gr.HTML(embed_html2,visible=True),htm3:gr.HTML(embed_html3,visible=False),input:gr.Image(sources=[\"webcam\"], streaming=True,label=\"Camera de surveillance\",visible=False),c1:gr.Column(scale=1,visible=True),c2:gr.Column(scale=1,visible=False)}  \n",
    "  elif j==\"CAMERA 3\":\n",
    "    return {htm1:gr.HTML(embed_html1,visible=False),htm2:gr.HTML(embed_html2,visible=False),htm3:gr.HTML(embed_html3,visible=True),input:gr.Image(sources=[\"webcam\"], streaming=True,label=\"Camera de surveillance\",visible=False),c1:gr.Column(scale=1,visible=False),c2:gr.Column(scale=1,visible=True)}\n",
    "  elif j==\"CAMERA 1\":\n",
    "    return {htm1:gr.HTML(embed_html1,visible=False),htm2:gr.HTML(embed_html2,visible=False),htm3:gr.HTML(embed_html3,visible=False),input:gr.Image(sources=[\"webcam\"], streaming=True,label=\"Camera de surveillance\",visible=True),c1:gr.Column(scale=1,visible=False),c2:gr.Column(scale=1,visible=True)}\n",
    "  else:\n",
    "    return {htm1:gr.HTML(embed_html1,visible=True),htm2:gr.HTML(embed_html2,visible=True),htm3:gr.HTML(embed_html3,visible=True),input:gr.Image(sources=[\"webcam\"], streaming=True,label=\"Camera de surveillance\",visible=True),c1:gr.Column(scale=1,visible=True),c2:gr.Column(scale=1,visible=True)}\n",
    "\n",
    "def fn2(j):\n",
    "  if j==\"CAMERA 4\":\n",
    "    return {htm1:gr.HTML(embed_html1,visible=True),htm2:gr.HTML(embed_html2,visible=False),htm3:gr.HTML(embed_html3,visible=False),inputvd:gr.Video(label=\"Camera de surveillance\",visible=False),c1:gr.Column(scale=1,visible=True),c2:gr.Column(scale=1,visible=False)}\n",
    "  elif j==\"CAMERA 2\":\n",
    "    return {htm1:gr.HTML(embed_html1,visible=False),htm2:gr.HTML(embed_html2,visible=True),htm3:gr.HTML(embed_html3,visible=False),inputvd:gr.Video(label=\"Camera de surveillance\",visible=False),c1:gr.Column(scale=1,visible=True),c2:gr.Column(scale=1,visible=False)}  \n",
    "  elif j==\"CAMERA 3\":\n",
    "    return {htm1:gr.HTML(embed_html1,visible=False),htm2:gr.HTML(embed_html2,visible=False),htm3:gr.HTML(embed_html3,visible=True),inputvd:gr.Video(label=\"Camera de surveillance\",visible=False),c1:gr.Column(scale=1,visible=False),c2:gr.Column(scale=1,visible=True)}\n",
    "  elif j==\"CAMERA 1\":\n",
    "    return {htm1:gr.HTML(embed_html1,visible=False),htm2:gr.HTML(embed_html2,visible=False),htm3:gr.HTML(embed_html3,visible=False),inputvd:gr.Video(label=\"Camera de surveillance\",visible=True),c1:gr.Column(scale=1,visible=False),c2:gr.Column(scale=1,visible=True)}\n",
    "  else:\n",
    "    return {htm1:gr.HTML(embed_html1,visible=True),htm2:gr.HTML(embed_html2,visible=True),htm3:gr.HTML(embed_html3,visible=True),inputvd:gr.Video(label=\"Camera de surveillance\",visible=True),c1:gr.Column(scale=1,visible=True),c2:gr.Column(scale=1,visible=True)}\n",
    "\n",
    "\n",
    "with gr.Blocks(css=custom_css) as demo:\n",
    "\n",
    "  with gr.Tab(\" SYSTEME DE DETECTION DE VIOLENCE\"):\n",
    "    with gr.Row(equal_height=True):\n",
    "        with gr.Column(scale=1) as c2:  \n",
    "          inputvd=gr.Video(sources='upload',include_audio=False,elem_id=\"Detection\",elem_classes=\"image-upload2\")\n",
    "          oup=gr.Image(height=500,elem_id=\"Detection\",visible=False,elem_classes=\"image-upload2\")#\n",
    "          # input=gr.Image(sources=[\"webcam\"], streaming=True,label=\"Camera de surveillance\",elem_id=\"Detection\",elem_classes=\"image-upload2\") \n",
    "          htm3=gr.HTML(embed_html3)\n",
    "        with gr.Column(scale=1) as c1:\n",
    "          htm1=gr.HTML(embed_html1)\n",
    "          htm2=gr.HTML(embed_html2)\n",
    "    \n",
    "        with gr.Column(scale=0.5):\n",
    "            output=gr.Textbox(label=\"ETAT\",value=\"Non Violence\",elem_id=\"Detection\")\n",
    "            probabilite=gr.Plot(elem_id=\"Detection\")#gr.Textbox(label=\"prob\",value=\"0\")\n",
    "            gr.Markdown(\n",
    "      \"\"\"\n",
    "      ## CAPTURE DE VIOLENCE\n",
    "      \"\"\")\n",
    "            output3=gr.Image(height=200,elem_id=\"Detection\")\n",
    "            gr.Markdown(\n",
    "      \"\"\"\n",
    "      ## ZOOM CAMERA\n",
    "      \"\"\")\n",
    "            with gr.Row():\n",
    "              \n",
    "              radio=gr.Radio([\"CAMERA 1\",\"CAMERA 2\", \"CAMERA 3\",\"CAMERA 4\",\"VUE GLOBALE\"],elem_id=\"Detection\")\n",
    "              radio.change(fn=fn2, inputs=radio, outputs=[htm1,htm2,htm3,inputvd,c1,c2])\n",
    "            examples = gr.Examples(examples=[\"DEMO/Fighting028_x264.mp4\"],inputs=inputvd)#\"DEMO/Normal_Videos_015_x264.mp4\",\"\n",
    "        output2 = gr.HTML()\n",
    "        output2.visible = False\n",
    "        inputvd.change(fn= vd,inputs= [inputvd],outputs=[inputvd,output,oup,output2,probabilite,output3])\n",
    "        \n",
    "        # gr.update(inputvd,visible=False)\n",
    "        # gr.update(oup,visible=True)\n",
    "  with gr.Tab(\"EXTRACTION D'IMAGE CLES\"):\n",
    "\n",
    "    with gr.Row(equal_height=True):\n",
    "      \n",
    "      with gr.Column():\n",
    "        gr.Markdown(\n",
    "    \"\"\"\n",
    "    # Video upload\n",
    "    \"\"\")\n",
    "        vido=gr.Video(sources='upload',include_audio=False,elem_id=\"Detection\",autoplay=True)#,example=[\"C:/Users/mayah/Desktop/videos_test/a.mpg\"]\n",
    "        # slid= gr.Slider(1,5, value=5, step=1, label=\"Parcourir la video:\")\n",
    "     \n",
    "        thr=gr.Number(label=\"Methode 1: Seuil\",elem_id=\"Detection\")\n",
    "        constraint=gr.Number(label=\"Methode 2: Nombre Ã  extraire\",elem_id=\"Detection\")\n",
    "        # slid.change(slidee,slid,[])\n",
    "        btn=gr.Button(\"GENERER\",elem_classes=\"btn\")\n",
    "  \n",
    "      with gr.Column():\n",
    "        gr.Markdown(\n",
    "    \"\"\"\n",
    "    # Images cles avec seuillage\n",
    "    \"\"\")\n",
    "        gallery = gr.Gallery(\n",
    "            label=\"Generated images\", show_label=False\n",
    "        , columns=[3], rows=[4], object_fit=\"fit\",height=\"290px\",elem_id=\"Detection\")\n",
    "        gr.Markdown(\n",
    "    \"\"\"\n",
    "    # Images cles avec clustering\n",
    "    \"\"\")\n",
    "        gallery2 = gr.Gallery(\n",
    "            label=\"Generated images\", show_label=False\n",
    "        , columns=[3], rows=[4], object_fit=\"fit\",height=\"290px\",elem_id=\"Detection\")\n",
    "        btn.click(fn= Extract_key_frames,inputs= [vido,thr,constraint],outputs=[gallery,gallery2])\n",
    "        examples = gr.Examples(examples=[\"DEMO/test2.mp4\"],inputs=vido)\n",
    "demo.queue()\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
